{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k3zTKjPSXjm"
   },
   "source": [
    "*-- Author: NORA --*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXTVxIXysM9O"
   },
   "source": [
    "## TEXT CLASSIFICATION  FOR  BBC NEWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-Nv6UldrVuXv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msztaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msztaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\msztaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import operator\n",
    "import requests\n",
    "nltk.download('stopwords') # If needed\n",
    "nltk.download('punkt') # If needed\n",
    "nltk.download('wordnet') # If needed\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9537f519d39c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/Nora/bbc\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "#path = \"/Nora/bbc\" # enter the location of folder name bbc\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLX-9_KS2Xer"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "vR3p8oif2EC6"
   },
   "source": [
    "os.chdir('business')\n",
    "#print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# List all files in a directory using os.listdir\n",
    "#basepath = cwd #'C:\\Users\\msztaj\\Downloads\\Nora\\bbc'\n",
    "for entry in os.listdir(basepath):\n",
    "    if os.path.isfile(os.path.join(basepath, entry)):\n",
    "        print(entry)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "path = r'C:\\Users\\msztaj\\Downloads\\Nora\\bbc\\business' # use your path\n",
    "all_files = glob.glob(path + \"/*.txt\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_json(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Business files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Ad sales boost Time Warner profit\\n\\nQuarterly...\n",
       "1  002  Dollar gains on Greenspan speech\\n\\nThe dollar...\n",
       "2  003  Yukos unit buyer faces loan claim\\n\\nThe owner...\n",
       "3  004  High fuel prices hit BA's profits\\n\\nBritish A...\n",
       "4  005  Pernod takeover talk lifts Domecq\\n\\nShares in..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./business\"):\n",
    "    filePath.append(os.path.join(\"./business\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_business = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_business.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Entertainment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Gallery unveils interactive tree\\n\\nA Christma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>Jarre joins fairytale celebration\\n\\nFrench mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Musical treatment for Capra film\\n\\nThe classi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>Richard and Judy choose top books\\n\\nThe 10 au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Poppins musical gets flying start\\n\\nThe stage...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Gallery unveils interactive tree\\n\\nA Christma...\n",
       "1  002  Jarre joins fairytale celebration\\n\\nFrench mu...\n",
       "2  003  Musical treatment for Capra film\\n\\nThe classi...\n",
       "3  004  Richard and Judy choose top books\\n\\nThe 10 au...\n",
       "4  005  Poppins musical gets flying start\\n\\nThe stage..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./entertainment\"):\n",
    "    filePath.append(os.path.join(\"./entertainment\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_entertainment = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_entertainment.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Politics files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Labour plans maternity pay rise\\n\\nMaternity p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>Watchdog probes e-mail deletions\\n\\nThe inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Hewitt decries 'career sexism'\\n\\nPlans to ext...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>Labour chooses Manchester\\n\\nThe Labour Party ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Brown ally rejects Budget spree\\n\\nChancellor ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Labour plans maternity pay rise\\n\\nMaternity p...\n",
       "1  002  Watchdog probes e-mail deletions\\n\\nThe inform...\n",
       "2  003  Hewitt decries 'career sexism'\\n\\nPlans to ext...\n",
       "3  004  Labour chooses Manchester\\n\\nThe Labour Party ...\n",
       "4  005  Brown ally rejects Budget spree\\n\\nChancellor ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./politics\"):\n",
    "    filePath.append(os.path.join(\"./politics\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_politics = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_politics.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Sports files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Claxton hunting first major medal\\n\\nBritish h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>O'Sullivan could run in Worlds\\n\\nSonia O'Sull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Greene sets sights on world title\\n\\nMaurice G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>IAAF launches fight against drugs\\n\\nThe IAAF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Dibaba breaks 5,000m world record\\n\\nEthiopia'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Claxton hunting first major medal\\n\\nBritish h...\n",
       "1  002  O'Sullivan could run in Worlds\\n\\nSonia O'Sull...\n",
       "2  003  Greene sets sights on world title\\n\\nMaurice G...\n",
       "3  004  IAAF launches fight against drugs\\n\\nThe IAAF ...\n",
       "4  005  Dibaba breaks 5,000m world record\\n\\nEthiopia'..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./sport\"):\n",
    "    filePath.append(os.path.join(\"./sport\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_sports = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_sports.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Tech files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Ink helps drive democracy in Asia\\n\\nThe Kyrgy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>China net cafe culture crackdown\\n\\nChinese au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Microsoft seeking spyware trojan\\n\\nMicrosoft ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>Digital guru floats sub-$100 PC\\n\\nNicholas Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Technology gets the creative bug\\n\\nThe hi-tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Ink helps drive democracy in Asia\\n\\nThe Kyrgy...\n",
       "1  002  China net cafe culture crackdown\\n\\nChinese au...\n",
       "2  003  Microsoft seeking spyware trojan\\n\\nMicrosoft ...\n",
       "3  004  Digital guru floats sub-$100 PC\\n\\nNicholas Ne...\n",
       "4  005  Technology gets the creative bug\\n\\nThe hi-tec..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./tech\"):\n",
    "    filePath.append(os.path.join(\"./tech\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_tech = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_tech.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding labels to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n",
      "386\n",
      "427\n",
      "511\n",
      "401\n"
     ]
    }
   ],
   "source": [
    "df_business['class'] = 'business'\n",
    "df_entertainment['class'] = 'entert'\n",
    "df_politics['class'] = 'polit'\n",
    "df_sports['class'] = 'sports'\n",
    "df_tech['class'] = 'tech'\n",
    "\n",
    "print(len(df_business.index))\n",
    "print(len(df_entertainment.index))\n",
    "print(len(df_politics.index))\n",
    "print(len(df_sports.index))\n",
    "print(len(df_tech.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tech\n",
    "# df_sports\n",
    "# df_politics\n",
    "# df_entertainment\n",
    "# df_business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w4XjUJfoI2Y_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Ad sales boost Time Warner profit\\n\\nQuarterly...\n",
       "1    Dollar gains on Greenspan speech\\n\\nThe dollar...\n",
       "2    Yukos unit buyer faces loan claim\\n\\nThe owner...\n",
       "3    High fuel prices hit BA's profits\\n\\nBritish A...\n",
       "4    Pernod takeover talk lifts Domecq\\n\\nShares in...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_business, df_entertainment, df_politics,df_sports, df_tech ])\n",
    "print(len(df.index))\n",
    "#df.head(5)\n",
    "\n",
    "training_data = pd.DataFrame(df, columns=['sentence', 'class'])\n",
    "training_data.to_csv(\"train_data.csv\", sep=',', encoding='utf-8')\n",
    "#print(training_data.sentence.shape)\n",
    "training_data.sentence.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27bceb78fd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df[\"category_id\"] = df[\"class\"].factorize()[0]\n",
    "#df.groupby('class').category_id.count().plot.bar(ylim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentence: 2235\n",
      "\n",
      "doc\n",
      "sentence\n",
      "class\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>bt program to beat dialler scams bt is introdu...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>spam e mails tempt net shoppers computer users...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>us cyber security chief resigns the man making...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>401</td>\n",
       "      <td>losing yourself in online gaming online role p...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc                                           sentence class\n",
       "396  397  bt program to beat dialler scams bt is introdu...  tech\n",
       "397  398  spam e mails tempt net shoppers computer users...  tech\n",
       "398  399  be careful how you code a new european directi...  tech\n",
       "399  400  us cyber security chief resigns the man making...  tech\n",
       "400  401  losing yourself in online gaming online role p...  tech"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Converting to lower-case\n",
    "Removing punctuations and numbers\n",
    "Remove blank spaces\n",
    "'''\n",
    "# To convert into lowercase\n",
    "df.loc[:,\"sentence\"] = df.sentence.apply(lambda x : str.lower(x))\n",
    "\n",
    "# To remove punctuation and numbers\n",
    "#import re\n",
    "df.loc[:,\"sentence\"] = df.sentence.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    "\n",
    "# To remove white spaces in the text\n",
    "# -----------------------------------\n",
    "print (\"Total Sentence: \"+str(len(df))+\"\\n\")\n",
    "for sentence in df[:1]:\n",
    "    sentence.strip()\n",
    "    print (sentence) # or print (patient_line.strip())\n",
    "\n",
    "#result = input_str.translate(string.maketrans(“”,””), string.punctuation)\n",
    "#print(result)\n",
    "\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# First, we get the stopwords list from nltk\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "# We can add more words to the stopword list, like punctuation marks\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "\n",
    "sentences = df['sentence']\n",
    "# Now we create a frequency dictionary with all words in the dataset\n",
    "# This can take a few minutes depending on your computer, since we are processing more than ten thousand sentences\n",
    "\n",
    "dict_word_frequency={}\n",
    "for sentence in sentences:\n",
    "  sentence_tokens = get_list_tokens(sentence)\n",
    "  for word in sentence_tokens:\n",
    "    if word in stopwords: continue\n",
    "    if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "    else: dict_word_frequency[word]+=1\n",
    " \n",
    "\n",
    "# Now we create a sorted frequency list with the top 1000 words, using the function \"sorted\". Let's see the 15 most frequent words\n",
    "sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:2000]\n",
    "i=0\n",
    "for word,frequency in sorted_list[:15]:\n",
    "  i+=1\n",
    "  print (str(i)+\". \"+word+\" - \"+str(frequency))\n",
    "  \n",
    "# Finally, we create our vocabulary based on the sorted frequency list \n",
    "vocabulary=[]\n",
    "for word,frequency in sorted_list:\n",
    "  vocabulary.append(word)\n",
    "\n",
    "sorted_list\n",
    "\n",
    "#dict_word_frequency\n",
    "#vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://www.geeksforgeeks.org/python-lemmatization-with-nltk/https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
    "\n",
    "#### https://www.geeksforgeeks.org/python-nltk-nltk-tokenize-tabtokenizer/?ref=rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term freqency based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#GET VECTOR COUNT\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data.sentence)\n",
    "#print(X_train_counts)\n",
    "\n",
    "#SAVE WORD VECTOR\n",
    "pickle.dump(count_vect.vocabulary_, open(\"count_vector.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2235x29421 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 451409 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#TRANSFORM WORD VECTOR TO TF IDF\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#SAVE TF-IDF\n",
    "pickle.dump(tfidf_transformer, open(\"tfidf.pkl\",\"wb\"))\n",
    "\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<559x29421 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 110941 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data['class'], test_size=0.25, random_state=42)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION USING Chi-Square testing\n",
    "#### for feature seleciton based on association of eahc words with one of five categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1676x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 42317 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The process of feature selection consists of selecting a subset of relevant features. \n",
    "In this project, we are going to use the [chi-squared test]\n",
    "(https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) method, \n",
    "available in sklearn. This method basically removes the features that appear to be irrelevant to \n",
    "a given class (in our one of five vcateogries). \n",
    "\n",
    "''' \n",
    "                                                            \n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "fs_chisq = SelectKBest(chi2, k=500).fit(X_train, y_train)\n",
    "X_train_chisq_new = fs_chisq.transform(X_train)\n",
    "\n",
    "X_train_chisq_new\n",
    "X_train_new = SelectKBest(chi2, k=500).fit_transform(X_train, y_train)\n",
    "X_train_new\n",
    "#print (\"Size original training matrix: \"+str(X_train_sentanalysis.shape))\n",
    "#print (\"Size new training matrix: \"+str(X_train_sentanalysis_new.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selectiom using tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2235, 118606)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "  \n",
    "tfidf = TfidfVectorizer(min_df = 2, max_df = 0.5, ngram_range = (1, 2))\n",
    "features = tfidf.fit_transform(df[\"sentence\"])\n",
    "\n",
    "tfidf = pd.DataFrame (features.todense(), columns = tfidf.get_feature_names())\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1676, 4193)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.feature_importances_  \n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X_train)\n",
    "X_new.shape               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate feature selection (Chi-Square test)\n",
    "##### For feature seleciton based on association of eahc words with one of five categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1676x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 42317 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The process of feature selection consists of selecting a subset of relevant features. \n",
    "In this project, we are going to use the [chi-squared test]\n",
    "(https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) method, \n",
    "available in sklearn. This method basically removes the features that appear to be irrelevant to \n",
    "a given class (in our one of five vcateogries). \n",
    "\n",
    "''' \n",
    "                                                            \n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "fs_chisq = SelectKBest(chi2, k=500).fit(X_train, y_train)\n",
    "X_train_chisq_new = fs_chisq.transform(X_train)\n",
    "\n",
    "X_train_chisq_new\n",
    "X_train_new = SelectKBest(chi2, k=500).fit_transform(X_train, y_train)\n",
    "X_train_new\n",
    "#print (\"Size original training matrix: \"+str(X_train_sentanalysis.shape))\n",
    "#print (\"Size new training matrix: \"+str(X_train_sentanalysis_new.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py:92: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1676, 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This approach not working\n",
    "'''\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X_train)\n",
    "X_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Very time consuming\n",
    "'''\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "sfs = SequentialFeatureSelector(knn, n_features_to_select = 5)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "SequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3), n_features_to_select=3)\n",
    "sfs.get_support()\n",
    "\n",
    "sfs.transform(X).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for News Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#clf = MultinomialNB().fit(X_train_tfidf, training_data.flag)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data['class'], test_size=0.25, random_state=42)\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "print(clf)\n",
    "\n",
    "# SAVE MODEL\n",
    "pickle.dump(clf, open(\"nb_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting for new data using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "category_list = [\"business\",\"entertainment\", \"politics\",\"sport\",  \"tech\"]\n",
    "\n",
    "docs_new = \"Barack Obama is viisting Canada for election reasons\"\n",
    "docs_new = [docs_new]\n",
    "\n",
    "#LOAD MODEL\n",
    "loaded_vec = CountVectorizer(vocabulary=pickle.load(open(\"count_vector.pkl\", \"rb\")))\n",
    "loaded_tfidf = pickle.load(open(\"tfidf.pkl\",\"rb\"))\n",
    "loaded_model = pickle.load(open(\"nb_model.pkl\",\"rb\"))\n",
    "\n",
    "X_new_counts = loaded_vec.transform(docs_new)\n",
    "X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "predicted = loaded_model.predict(X_new_tfidf)\n",
    "\n",
    "predicted\n",
    "#print(category_list[predicted[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6fa0eecc20d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresult_bayes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'true_labels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'predicted_labels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresult_bayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'res_bayes.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult_bayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "predicted = loaded_model.predict(X_test)\n",
    "result_bayes = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_bayes.to_csv('res_bayes.csv', sep = ',')\n",
    "\n",
    "result_bayes.head(5)\n",
    "#for predicted_item, result in zip(predicted, y_test):\n",
    "#    print(category_list[predicted_item], ' - ', category_list[result])\n",
    "\n",
    "\n",
    "'''\n",
    "Confusion matrix\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "confusion_mat = confusion_matrix(y_test,predicted)\n",
    "print(confusion_mat)\n",
    "\n",
    "\n",
    "'''\n",
    "Precision, Recall and F score\n",
    "'''\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer percentpron Netowrk for News Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_neural = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data[\"class\"], test_size=0.25, random_state=42)\n",
    "\n",
    "clf_neural.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf_neural, open(\"softmax.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf_neural.predict(X_test)\n",
    "result_softmax = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_softmax.to_csv('res_softmax.csv', sep = ',')\n",
    "result_softmax.head(5)\n",
    "#for predicted_item, result in zip(predicted, y_test):\n",
    "#    print(category_list[predicted_item], ' - ', category_list[result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(y_test,predicted)\n",
    "print(confusion_mat)\n",
    "\n",
    "'''\n",
    "Precision, Recall and F score\n",
    "'''\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Classifier for News classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf_svm = svm.LinearSVC()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data[\"class\"], test_size=0.25, random_state=42)\n",
    "clf_svm.fit(X_train_tfidf, training_data[\"class\"])\n",
    "pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf_svm.predict(X_test)\n",
    "result_svm = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_svm.to_csv('res_svm.csv', sep = ',')\n",
    "#for predicted_item, result in zip(predicted, y_test):\n",
    "#    print(category_list[predicted_item], ' - ', category_list[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(y_test,predicted)\n",
    "print(confusion_mat)\n",
    "\n",
    "'''\n",
    "Precision, Recall and F score\n",
    "'''\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRACK RETURNED FROM TRIALS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Function taken from Session 1\n",
    "def get_list_tokens(string):\n",
    "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "  list_tokens=[]\n",
    "  for sentence in sentence_split:\n",
    "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "    for token in list_tokens_sentence:\n",
    "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "  return list_tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_vector_text(list_vocab,string):\n",
    "  vector_text=np.zeros(len(list_vocab))\n",
    "  list_tokens_string=get_list_tokens(string)\n",
    "  for i, word in enumerate(list_vocab):\n",
    "    if word in list_tokens_string:\n",
    "      vector_text[i]=list_tokens_string.count(word)\n",
    "  return vector_text\n",
    "\n",
    "#get_vector_text(vocabulary)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import these modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "   \n",
    "ps = PorterStemmer()\n",
    "  \n",
    "# choose some words to be stemmed\n",
    "words = \"running\"\n",
    "  \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Training and testing datasets\n",
    "### ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "count the number of categories in each split\n",
    "'''\n",
    "\n",
    "train, validate, test = \\\n",
    "              np.split(df.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "print(len(train.index))\n",
    "print(len(test.index))\n",
    "print(len(validate.index))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "Count the number of categories in each split\n",
    "'''\n",
    "\n",
    "print(train.groupby('class').count())\n",
    "print(validate.groupby('class').count())\n",
    "print(test.groupby('class').count())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "-sKz-jCdvOuU"
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "guExdzmCWpu7"
   },
   "source": [
    "Once we preprocessed the data, we are ready to train our first machine learning algorithm! In this case we are going to use an SVM binary classifier (we will see more details about machine learning algorithms from Session 4). As a binary classifier, for training we should provide the features as input and \"1\" or \"0 as output. The function to train a machine learning model in sklearn is `.fit`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "hJlXph8NWqsn"
   },
   "source": [
    "X_train_diabetes=np.asarray(X_train)\n",
    "Y_train_diabetes=np.asarray(Y_train) # This step is really not necessary, but it is recommended to work with numpy arrays instead of Python lists.\n",
    "\n",
    "svm_clf_diabetes=sklearn.svm.SVC(kernel=\"linear\",gamma='auto') # Initialize the SVM model\n",
    "svm_clf_diabetes.fit(X_train_diabetes,Y_train_diabetes) # Train the SVM model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "sJOfaaWENQwj"
   },
   "source": [
    "We have already trained our first supervised machine learning classifier! Let's check now how it works with two random patients:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "wprWeeVANZZm"
   },
   "source": [
    "patient_1=['0', '100', '86', '20', '39', '35.1', '0.242', '21']\n",
    "patient_2=['1', '197', '70', '45', '543', '30.5', '0.158', '51']\n",
    "print (svm_clf_diabetes.predict([patient_1]))\n",
    "print (svm_clf_diabetes.predict([patient_2]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "RAke3d5QIAJS"
   },
   "source": [
    "**Excercise 1:**\n",
    "Choose three features from the eight features of the \"Diabetes\" dataset and learn the same binary SVM classifier. Check how the classifier works with one example, i.e., choose random values for your three features and check the prediction of your SVM classifier."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "wIAxWjDxZhuV"
   },
   "source": [
    "X_train=[]\n",
    "Y_train=[]\n",
    "#To complete"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E7m0tAb_-aTq"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In Machine Learning, the process of feature engineering consists of transforming data into features. In the previous examples with the \"Diabetes\" dataset the features were already given, but in most cases we should extract the features ourselves. In this case, we are going to deal with examples with textual data. To extract features from textual content, we can make use of what we learned from the exercises of Session 1.\n",
    "\n",
    "For these exercises we will be using a dataset for *sentiment analysis*. Sentiment analysis is the automatic process of classifying opinions as positive or negative (there are other definitions of sentiment analysis which are more general as well). To do so, we are going to make use of the RT-polarity dataset. Let's first download it and inspect the data. This time we are going to load the dataset directly from the web, but feel free to use the method of your choice to load the data, as explained above: \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "IRlEgUcvIw_f"
   },
   "source": [
    "#Load positive reviews\n",
    "response_pos = requests.get(url_pos)\n",
    "dataset_file_pos = response_pos.text.split(\"\\n\")\n",
    "\n",
    "#Load negative reviews\n",
    "response_neg = requests.get(url_neg)\n",
    "dataset_file_neg = response_neg.text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "vln_3gb4zV_y"
   },
   "source": [
    "Let's inspect a bit the dataset, by printing the first five positive and negative reviews."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "VAFJ2MKszXQK"
   },
   "source": [
    "print (\"Positive reviews:\\n\")\n",
    "for pos_review in dataset_file_pos[:5]:\n",
    "  print (pos_review)\n",
    "print (\"\\n   ------\\n\")  \n",
    "print (\"Negative reviews:\\n\")\n",
    "for neg_review in dataset_file_neg[:5]:\n",
    "  print (neg_review)\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "rggiZRURumOu"
   },
   "source": [
    "Now we are going to try to define a vocabulary which can be used to transform sentences (strings) into text. Let's take, for example, the 1000 most frequent words in the dataset, excluding stopwords.\n",
    "\n",
    "**Note:** Stopwords are generally short function words that do not provide a specific meaning without context (e.g. articles such as \"the\" or prepositions such as \"on\"). They can be different depending on the purpose. In our case we will be using the English stopwords as given by NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "BaLXuRezxrwJ"
   },
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Function taken from Session 1\n",
    "def get_list_tokens(string):\n",
    "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "  list_tokens=[]\n",
    "  for sentence in sentence_split:\n",
    "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "    for token in list_tokens_sentence:\n",
    "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "  return list_tokens"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "FGgaqwNuUwLd"
   },
   "source": [
    "# First, we get the stopwords list from nltk\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "# We can add more words to the stopword list, like punctuation marks\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "\n",
    "# Now we create a frequency dictionary with all words in the dataset\n",
    "# This can take a few minutes depending on your computer, since we are processing more than ten thousand sentences\n",
    "\n",
    "dict_word_frequency={}\n",
    "for sentence in df:\n",
    "  sentence_tokens=get_list_tokens(sentence)\n",
    "  for word in sentence_tokens:\n",
    "    if word in stopwords: continue\n",
    "    if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "    else: dict_word_frequency[word]+=1\n",
    "      \n",
    "# Now we create a sorted frequency list with the top 1000 words, using the function \"sorted\". Let's see the 15 most frequent words\n",
    "sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:1000]\n",
    "i=0\n",
    "for word,frequency in sorted_list[:15]:\n",
    "  i+=1\n",
    "  print (str(i)+\". \"+word+\" - \"+str(frequency))\n",
    "  \n",
    "# Finally, we create our vocabulary based on the sorted frequency list \n",
    "vocabulary=[]\n",
    "for word,frequency in sorted_list:\n",
    "  vocabulary.append(word)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "9jB5b2IS8ZzK"
   },
   "source": [
    "Once we have our vocabulary, we can transform sentences into vectors as we saw in Session 1, using the function below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "qZrrwpmUud2Y"
   },
   "source": [
    "def get_vector_text(list_vocab,string):\n",
    "  vector_text=np.zeros(len(list_vocab))\n",
    "  list_tokens_string=get_list_tokens(string)\n",
    "  for i, word in enumerate(list_vocab):\n",
    "    if word in list_tokens_string:\n",
    "      vector_text[i]=list_tokens_string.count(word)\n",
    "  return vector_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "euSGxH85-r5z"
   },
   "source": [
    "Using this function we can now load our training features, as we did with the \"Diabetes\" dataset. In this case, we will label positive reviews as \"1\" and negative reviews as \"0\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HNMPj692-uU0"
   },
   "source": [
    "# This can take a while, as we are converting more than ten thousand sentences into vectors!\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "for pos_review in dataset_file_pos:\n",
    "  vector_pos_review=get_vector_text(vocabulary,pos_review)\n",
    "  X_train.append(vector_pos_review)\n",
    "  Y_train.append(1)\n",
    "for neg_review in dataset_file_neg:\n",
    "  vector_neg_review=get_vector_text(vocabulary,neg_review)\n",
    "  X_train.append(vector_neg_review)\n",
    "  Y_train.append(0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ge5heo2N-9m8"
   },
   "source": [
    "**Exercise (optional):** Try transforming the sentences into weighted frequency features using [TFidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). This function uses a weighted scheme called [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (term frequency-inverse document frequency) which basically penalizes words that are repeated across many documents (e.g. frequent words such as \"the\" or \"a\")."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "CYlpPJlz-NxK"
   },
   "source": [
    "Once we have loaded all the feature vectors, we can now train our SVM binary classifier! "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "0eRzTNWv8h0t"
   },
   "source": [
    "X_train_sentanalysis=np.asarray(X_train)\n",
    "Y_train_sentanalysis=np.asarray(Y_train)\n",
    "\n",
    "svm_clf_sentanalysis=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf_sentanalysis.fit(X_train_sentanalysis,Y_train_sentanalysis) # Train the SVM model. This may also take a while.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "L6ReLfgDyNbQ"
   },
   "source": [
    "Let's try how it works with some examples!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "mH4VcPPXAhdM"
   },
   "source": [
    "sentence_1=\"Fascinating, I loved it.\"\n",
    "sentence_2=\"Bad movie, probably one of the worst I have ever seen.\"\n",
    "print (svm_clf_sentanalysis.predict([get_vector_text(vocabulary,sentence_1)]))\n",
    "print (svm_clf_sentanalysis.predict([get_vector_text(vocabulary,sentence_2)]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "BdrfQRRuAh28"
   },
   "source": [
    "It seems to be working! However, this is a very simple classifier and is definetely not perfect. You can try other examples yourself to see how the model behaves, find weaknesses and try to improve it with better features!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "_lP9GC-tEDcM"
   },
   "source": [
    "**Excercise 2:**\n",
    "Based on this example, create a function that, given two files of positive and negative reviews (one sentence per line as in our RT-polarity dataset) and an integer number X as input, it returns the vocabulary and a binary SVM classifier similar to what we learned, using the X most frequent words as features. Check how the classifier works with X=1200. You can check the predictions with the same sample sentences as above.\n",
    "\n",
    "**Note:** You can use auxiliary functions if needed (not mandatory but can be useful). For example, a function that first retrieves the vocabulary given the datasets and X."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "dJcDFw7iFBvR"
   },
   "source": [
    "def train_svm_classifier(dataset_file_pos, dataset_file_neg, x):\n",
    "  #To complete"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ObiYSdEYvk_t"
   },
   "source": [
    "**Exercise (optional):** Think about different features that can be useful for sentiment analysis and add it to our frequency vector. Some ideas: (1) use a dictionary of positive or negative words (some dictionaries available [here](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)); (2) use n-gram features (n-grams are sequence of n-words as opposed or a single word, e.g., \"cardiff university\" would be a bigram); (3) use only verbs and adjectives as features (see [PoS tagging](https://www.nltk.org/book/ch05.html) in NLTK)..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "5OBMzQ8cB8C7"
   },
   "source": [
    "\n",
    "## Feature selection\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The process of feature selection consists of selecting a subset of relevant features. For example, in the sentiment analysis example above, we selected 1000 features based on the 1000 most frequent words. However, not all words may be equally relevant. For example, \"film\" is the second most frequent word but may appear equally in positive and negative reviews, therefore it is not a very relevant feature for our task.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "sRSPZN1CGD_o"
   },
   "source": [
    "In this notebook we are going to use the [chi-squared test](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) method, available in sklearn. This method basically removes the features that appear to be irrelevant to a given class (in our case positive or negative). For example, words that do not express sentiment are expected to be removed from the set. Let's apply this feature selection method to our RT-polarity dataset to keep only the 500 most relevant features."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "w-0HOolhHnxV"
   },
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "9wKZ2b4vgquE"
   },
   "source": [
    "fs_sentanalysis=SelectKBest(chi2, k=500).fit(X_train_sentanalysis, Y_train_sentanalysis)\n",
    "X_train_sentanalysis_new = fs_sentanalysis.transform(X_train_sentanalysis)\n",
    "#X_train_new = SelectKBest(chi2, k=500).fit_transform(X_train, Y_train)\n",
    "print (\"Size original training matrix: \"+str(X_train_sentanalysis.shape))\n",
    "print (\"Size new training matrix: \"+str(X_train_sentanalysis_new.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "kp34c7NTJxP2"
   },
   "source": [
    "Now we can train again our SVM classifier with the 500 most relevant features, replacing the old one."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "BDl987FvH6IX"
   },
   "source": [
    "svm_clf_sentanalysis_=sklearn.svm.SVC(kernel=\"linear\",gamma='auto') # Change the name here, e.g. 'new sentanalysis_svm_clf', and below if you don't want to replace your old classifier.\n",
    "svm_clf_sentanalysis_.fit(X_train_sentanalysis_new,Y_train_sentanalysis) #Train the new SVM model. This may take a while."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "5_n4Q6DtKP78"
   },
   "source": [
    "And now we can test our classifier with some new examples.\n",
    "\n",
    "**Note**: To transform the original 1000 features into our reduced 500 features, we use the function `.transform`. This function is very common in sklearn."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "EdAXTa30IYd4"
   },
   "source": [
    "sentence_3=\"Highly recommended: it was a fascinating film.\"\n",
    "sentence_4=\"I got a bit bored, it was not what I was expecting.\"\n",
    "print (svm_clf_sentanalysis_.predict(fs_sentanalysis.transform([get_vector_text(vocabulary,sentence_3)])))\n",
    "print (svm_clf_sentanalysis_.predict(fs_sentanalysis.transform([get_vector_text(vocabulary,sentence_4)])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MejtAgMhON64"
   },
   "source": [
    "**Exercise 3:** Apply the same chi-squared feature selection method to select the seven most relevant features from the Diabetes dataset. Check your method with some sample input features (you can use the same \"patient_1\" and \"patient_2\" examples)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "RlHAEcTNP9eV"
   },
   "source": [
    "# To complete"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "awsktnSuKZ_W"
   },
   "source": [
    "**Exercise (optional):** Check other feature selection methods in skelarn (feature selection methods available [here](https://scikit-learn.org/stable/modules/feature_selection.html)) and try one of them with our sentiment analysis dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FeatureEngineeringSelection_Sklearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
