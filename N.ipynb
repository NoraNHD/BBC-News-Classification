{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k3zTKjPSXjm"
   },
   "source": [
    "*-- Author: NORA --*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXTVxIXysM9O"
   },
   "source": [
    "## TEXT CLASSIFICATION  FOR  BBC NEWS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-Nv6UldrVuXv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msztaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msztaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\msztaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import operator\n",
    "import requests\n",
    "nltk.download('stopwords') # If needed\n",
    "nltk.download('punkt') # If needed\n",
    "nltk.download('wordnet') # If needed\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLX-9_KS2Xer"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9537f519d39c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/Nora/bbc\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "#path = \"/Nora/bbc\" # enter the location of folder name bbc\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Business files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Ad sales boost Time Warner profit\\n\\nQuarterly...\n",
       "1  002  Dollar gains on Greenspan speech\\n\\nThe dollar...\n",
       "2  003  Yukos unit buyer faces loan claim\\n\\nThe owner...\n",
       "3  004  High fuel prices hit BA's profits\\n\\nBritish A...\n",
       "4  005  Pernod takeover talk lifts Domecq\\n\\nShares in..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./business\"):\n",
    "    filePath.append(os.path.join(\"./business\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_business = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_business.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Entertainment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Gallery unveils interactive tree\\n\\nA Christma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>Jarre joins fairytale celebration\\n\\nFrench mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Musical treatment for Capra film\\n\\nThe classi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>Richard and Judy choose top books\\n\\nThe 10 au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Poppins musical gets flying start\\n\\nThe stage...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Gallery unveils interactive tree\\n\\nA Christma...\n",
       "1  002  Jarre joins fairytale celebration\\n\\nFrench mu...\n",
       "2  003  Musical treatment for Capra film\\n\\nThe classi...\n",
       "3  004  Richard and Judy choose top books\\n\\nThe 10 au...\n",
       "4  005  Poppins musical gets flying start\\n\\nThe stage..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./entertainment\"):\n",
    "    filePath.append(os.path.join(\"./entertainment\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_entertainment = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_entertainment.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Politics files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Labour plans maternity pay rise\\n\\nMaternity p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>Watchdog probes e-mail deletions\\n\\nThe inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Hewitt decries 'career sexism'\\n\\nPlans to ext...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>Labour chooses Manchester\\n\\nThe Labour Party ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Brown ally rejects Budget spree\\n\\nChancellor ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Labour plans maternity pay rise\\n\\nMaternity p...\n",
       "1  002  Watchdog probes e-mail deletions\\n\\nThe inform...\n",
       "2  003  Hewitt decries 'career sexism'\\n\\nPlans to ext...\n",
       "3  004  Labour chooses Manchester\\n\\nThe Labour Party ...\n",
       "4  005  Brown ally rejects Budget spree\\n\\nChancellor ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./politics\"):\n",
    "    filePath.append(os.path.join(\"./politics\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_politics = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_politics.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Sports files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Claxton hunting first major medal\\n\\nBritish h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>O'Sullivan could run in Worlds\\n\\nSonia O'Sull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Greene sets sights on world title\\n\\nMaurice G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>IAAF launches fight against drugs\\n\\nThe IAAF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Dibaba breaks 5,000m world record\\n\\nEthiopia'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Claxton hunting first major medal\\n\\nBritish h...\n",
       "1  002  O'Sullivan could run in Worlds\\n\\nSonia O'Sull...\n",
       "2  003  Greene sets sights on world title\\n\\nMaurice G...\n",
       "3  004  IAAF launches fight against drugs\\n\\nThe IAAF ...\n",
       "4  005  Dibaba breaks 5,000m world record\\n\\nEthiopia'..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./sport\"):\n",
    "    filePath.append(os.path.join(\"./sport\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_sports = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_sports.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Tech files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>Ink helps drive democracy in Asia\\n\\nThe Kyrgy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>China net cafe culture crackdown\\n\\nChinese au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>Microsoft seeking spyware trojan\\n\\nMicrosoft ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>Digital guru floats sub-$100 PC\\n\\nNicholas Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>Technology gets the creative bug\\n\\nThe hi-tec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc                                           sentence\n",
       "0  001  Ink helps drive democracy in Asia\\n\\nThe Kyrgy...\n",
       "1  002  China net cafe culture crackdown\\n\\nChinese au...\n",
       "2  003  Microsoft seeking spyware trojan\\n\\nMicrosoft ...\n",
       "3  004  Digital guru floats sub-$100 PC\\n\\nNicholas Ne...\n",
       "4  005  Technology gets the creative bug\\n\\nThe hi-tec..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/msztaj/Downloads/Nora/bbc\"\n",
    "os.chdir(path)\n",
    "\n",
    "#get full path of txt file\n",
    "filePath = []\n",
    "for file in os.listdir(\"./tech\"):\n",
    "    filePath.append(os.path.join(\"./tech\", file))\n",
    "\n",
    "#pull file name from text file with regex, capturing the text before the .txt   \n",
    "fileName = re.compile('\\\\\\\\(.*)\\.txt')\n",
    "\n",
    "#make empty dict Data with the key as the file name, and the value as the words in the file.\n",
    "data = {}\n",
    "for file in filePath:\n",
    "    #capturing file name\n",
    "    key = fileName.search(file)\n",
    "    with open(file, \"r\") as readFile:\n",
    "        # note that key[1] is the capture group from our search, and that the text is put into a list.\n",
    "        data[key[1]] = [readFile.read()]\n",
    "\n",
    "#make dataframe from dict, and rename columns.\n",
    "df_tech = pd.DataFrame(data).T.reset_index().rename(columns = {'index':'doc', 0:'sentence'})\n",
    "\n",
    "df_tech.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding labels to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n",
      "386\n",
      "427\n",
      "511\n",
      "401\n"
     ]
    }
   ],
   "source": [
    "df_business['class'] = 'business'\n",
    "df_entertainment['class'] = 'entert'\n",
    "df_politics['class'] = 'polit'\n",
    "df_sports['class'] = 'sports'\n",
    "df_tech['class'] = 'tech'\n",
    "\n",
    "print(len(df_business.index))\n",
    "print(len(df_entertainment.index))\n",
    "print(len(df_politics.index))\n",
    "print(len(df_sports.index))\n",
    "print(len(df_tech.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tech\n",
    "# df_sports\n",
    "# df_politics\n",
    "# df_entertainment\n",
    "# df_business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w4XjUJfoI2Y_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Ad sales boost Time Warner profit\\n\\nQuarterly...\n",
       "1    Dollar gains on Greenspan speech\\n\\nThe dollar...\n",
       "2    Yukos unit buyer faces loan claim\\n\\nThe owner...\n",
       "3    High fuel prices hit BA's profits\\n\\nBritish A...\n",
       "4    Pernod takeover talk lifts Domecq\\n\\nShares in...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_business, df_entertainment, df_politics,df_sports, df_tech ])\n",
    "print(len(df.index))\n",
    "#df.head(5)\n",
    "\n",
    "training_data = pd.DataFrame(df, columns=['sentence', 'class'])\n",
    "training_data.to_csv(\"train_data.csv\", sep=',', encoding='utf-8')\n",
    "#print(training_data.sentence.shape)\n",
    "training_data.sentence.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27bceb78fd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df[\"category_id\"] = df[\"class\"].factorize()[0]\n",
    "#df.groupby('class').category_id.count().plot.bar(ylim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentence: 2235\n",
      "\n",
      "doc\n",
      "sentence\n",
      "class\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>sentence</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>bt program to beat dialler scams bt is introdu...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>spam e mails tempt net shoppers computer users...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>us cyber security chief resigns the man making...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>401</td>\n",
       "      <td>losing yourself in online gaming online role p...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc                                           sentence class\n",
       "396  397  bt program to beat dialler scams bt is introdu...  tech\n",
       "397  398  spam e mails tempt net shoppers computer users...  tech\n",
       "398  399  be careful how you code a new european directi...  tech\n",
       "399  400  us cyber security chief resigns the man making...  tech\n",
       "400  401  losing yourself in online gaming online role p...  tech"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Converting to lower-case\n",
    "Removing punctuations and numbers\n",
    "Remove blank spaces\n",
    "'''\n",
    "# To convert into lowercase\n",
    "df.loc[:,\"sentence\"] = df.sentence.apply(lambda x : str.lower(x))\n",
    "\n",
    "# To remove punctuation and numbers\n",
    "#import re\n",
    "df.loc[:,\"sentence\"] = df.sentence.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    "\n",
    "# To remove white spaces in the text\n",
    "# -----------------------------------\n",
    "print (\"Total Sentence: \"+str(len(df))+\"\\n\")\n",
    "for sentence in df[:1]:\n",
    "    sentence.strip()\n",
    "    print (sentence) # or print (patient_line.strip())\n",
    "\n",
    "#result = input_str.translate(string.maketrans(“”,””), string.punctuation)\n",
    "#print(result)\n",
    "\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://www.geeksforgeeks.org/python-lemmatization-with-nltk/https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
    "\n",
    "#### https://www.geeksforgeeks.org/python-nltk-nltk-tokenize-tabtokenizer/?ref=rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term freqency based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#GET VECTOR COUNT\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data.sentence)\n",
    "#print(X_train_counts)\n",
    "\n",
    "#SAVE WORD VECTOR\n",
    "pickle.dump(count_vect.vocabulary_, open(\"count_vector.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2235x29421 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 451409 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#TRANSFORM WORD VECTOR TO TF IDF\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#SAVE TF-IDF\n",
    "pickle.dump(tfidf_transformer, open(\"tfidf.pkl\",\"wb\"))\n",
    "\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<559x29421 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 110941 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data['class'], test_size=0.25, random_state=42)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SELECTION USING Chi-Square testing\n",
    "#### for feature seleciton based on association of eahc words with one of five categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1676x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 42317 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The process of feature selection consists of selecting a subset of relevant features. \n",
    "In this project, we are going to use the [chi-squared test]\n",
    "(https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) method, \n",
    "available in sklearn. This method basically removes the features that appear to be irrelevant to \n",
    "a given class (in our one of five vcateogries). \n",
    "\n",
    "''' \n",
    "                                                            \n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "fs_chisq = SelectKBest(chi2, k=500).fit(X_train, y_train)\n",
    "X_train_chisq_new = fs_chisq.transform(X_train)\n",
    "\n",
    "X_train_chisq_new\n",
    "X_train_new = SelectKBest(chi2, k=500).fit_transform(X_train, y_train)\n",
    "X_train_new\n",
    "#print (\"Size original training matrix: \"+str(X_train_sentanalysis.shape))\n",
    "#print (\"Size new training matrix: \"+str(X_train_sentanalysis_new.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selectiom using tf-idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2235, 118606)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "  \n",
    "tfidf = TfidfVectorizer(min_df = 2, max_df = 0.5, ngram_range = (1, 2))\n",
    "features = tfidf.fit_transform(df[\"sentence\"])\n",
    "\n",
    "tfidf = pd.DataFrame (features.todense(), columns = tfidf.get_feature_names())\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1676, 4193)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.feature_importances_  \n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X_train)\n",
    "X_new.shape               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate feature selection (Chi-Square test)\n",
    "##### For feature seleciton based on association of eahc words with one of five categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1676x500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 42317 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The process of feature selection consists of selecting a subset of relevant features. \n",
    "In this project, we are going to use the [chi-squared test]\n",
    "(https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) method, \n",
    "available in sklearn. This method basically removes the features that appear to be irrelevant to \n",
    "a given class (in our one of five vcateogries). \n",
    "\n",
    "''' \n",
    "                                                            \n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "fs_chisq = SelectKBest(chi2, k=500).fit(X_train, y_train)\n",
    "X_train_chisq_new = fs_chisq.transform(X_train)\n",
    "\n",
    "X_train_chisq_new\n",
    "X_train_new = SelectKBest(chi2, k=500).fit_transform(X_train, y_train)\n",
    "X_train_new\n",
    "#print (\"Size original training matrix: \"+str(X_train_sentanalysis.shape))\n",
    "#print (\"Size new training matrix: \"+str(X_train_sentanalysis_new.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_base.py:92: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1676, 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This approach not working\n",
    "'''\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X_train, y_train)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X_train)\n",
    "X_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Very time consuming\n",
    "'''\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "sfs = SequentialFeatureSelector(knn, n_features_to_select = 5)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "SequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3), n_features_to_select=3)\n",
    "sfs.get_support()\n",
    "\n",
    "sfs.transform(X).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier for News Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#clf = MultinomialNB().fit(X_train_tfidf, training_data.flag)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data['class'], test_size=0.25, random_state=42)\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "print(clf)\n",
    "\n",
    "# SAVE MODEL\n",
    "pickle.dump(clf, open(\"nb_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting for new data using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "category_list = [\"business\",\"entertainment\", \"politics\",\"sport\",  \"tech\"]\n",
    "\n",
    "docs_new = \"Barack Obama is viisting Canada for election reasons\"\n",
    "docs_new = [docs_new]\n",
    "\n",
    "#LOAD MODEL\n",
    "loaded_vec = CountVectorizer(vocabulary=pickle.load(open(\"count_vector.pkl\", \"rb\")))\n",
    "loaded_tfidf = pickle.load(open(\"tfidf.pkl\",\"rb\"))\n",
    "loaded_model = pickle.load(open(\"nb_model.pkl\",\"rb\"))\n",
    "\n",
    "X_new_counts = loaded_vec.transform(docs_new)\n",
    "X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "predicted = loaded_model.predict(X_new_tfidf)\n",
    "\n",
    "predicted\n",
    "#print(category_list[predicted[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6fa0eecc20d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresult_bayes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'true_labels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'predicted_labels'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresult_bayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'res_bayes.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult_bayes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "predicted = loaded_model.predict(X_test)\n",
    "result_bayes = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_bayes.to_csv('res_bayes.csv', sep = ',')\n",
    "\n",
    "result_bayes.head(5)\n",
    "#for predicted_item, result in zip(predicted, y_test):\n",
    "#    print(category_list[predicted_item], ' - ', category_list[result])\n",
    "\n",
    "\n",
    "'''\n",
    "Confusion matrix\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "confusion_mat = confusion_matrix(y_test,predicted)\n",
    "print(confusion_mat)\n",
    "\n",
    "\n",
    "'''\n",
    "Precision, Recall and F score\n",
    "'''\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, predicted, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer percentpron Netowrk for News Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_neural = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data[\"class\"], test_size=0.25, random_state=42)\n",
    "\n",
    "clf_neural.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf_neural, open(\"softmax.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf_neural.predict(X_test)\n",
    "result_softmax = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_softmax.to_csv('res_softmax.csv', sep = ',')\n",
    "result_softmax.head(5)\n",
    "#for predicted_item, result in zip(predicted, y_test):\n",
    "#    print(category_list[predicted_item], ' - ', category_list[result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(y_test,predicted)\n",
    "print(confusion_mat)\n",
    "\n",
    "'''\n",
    "Precision, Recall and F score\n",
    "'''\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Classifier for News classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf_svm = svm.LinearSVC()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data[\"class\"], test_size=0.25, random_state=42)\n",
    "clf_svm.fit(X_train_tfidf, training_data[\"class\"])\n",
    "pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf_svm.predict(X_test)\n",
    "result_svm = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_svm.to_csv('res_svm.csv', sep = ',')\n",
    "#for predicted_item, result in zip(predicted, y_test):\n",
    "#    print(category_list[predicted_item], ' - ', category_list[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(y_test,predicted)\n",
    "print(confusion_mat)\n",
    "\n",
    "'''\n",
    "Precision, Recall and F score\n",
    "'''\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, predicted, average='weighted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "awsktnSuKZ_W"
   },
   "source": [
    "**Exercise (optional):** Check other feature selection methods in skelarn (feature selection methods available [here](https://scikit-learn.org/stable/modules/feature_selection.html)) and try one of them with our sentiment analysis dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FeatureEngineeringSelection_Sklearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
